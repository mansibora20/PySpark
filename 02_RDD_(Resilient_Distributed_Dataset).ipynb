{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK1ntzErm8cssw9zrA4REA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mansibora20/PySpark/blob/main/02_RDD_(Resilient_Distributed_Dataset).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNsQTD9UO7mt",
        "outputId": "b4accecf-c648-404b-d077-8c6809bcfc96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\t\t   spark-3.1.1-bin-hadoop3.2.tgz\n",
            "spark-3.1.1-bin-hadoop3.2  spark-3.1.1-bin-hadoop3.2.tgz.1\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB-9YMemO9u2",
        "outputId": "0541ff7e-6a1e-480e-8e62-65cfecf555fe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PceVoTppTBA8",
        "outputId": "27a4ba94-0c6c-42b4-ac4b-9a90c5c219e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (185.125.190.82)] [Waiting for headers] [Waiting for headers] \r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "2Hxk1oUAPP9Q"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNWV9b-QPpEZ",
        "outputId": "ec580cdc-2cba-4d62-dc05-ddb324a9d507"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\t\t   spark-3.1.1-bin-hadoop3.2.tgz    spark-3.1.1-bin-hadoop3.2.tgz.2\n",
            "spark-3.1.1-bin-hadoop3.2  spark-3.1.1-bin-hadoop3.2.tgz.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SparkContext**\n",
        "* SparkContext is used for low-level operations like RDDs, accumulators, and broadcast variables.\n",
        "* Can only be created once in an application\n",
        "* SparkContext is the legacy entry point for low-level operations.\n",
        "* It's used to access the underlying Spark environment and perform operations on it.\n",
        "* It's necessary when you need more control over the underlying Spark execution.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LHpxYGQrSRRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "#spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "E4sb7E5CSkA4",
        "outputId": "1d3f3d7c-b9c7-4a3b-f98a-0c16d77d1549"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://59c21cb966fa:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SparkSession**\n",
        "* SparkSession is used for high-level operations like DataFrame and SQL.\n",
        "* Can be created multiple times in an application.\n",
        "* SparkSession is the entry point to Spark SQL.\n",
        "* It offers a unified interface for interacting with Spark APIs.\n",
        "* It supports built-in integration with various Spark modules.\n",
        "* It's ideal for most data processing tasks because of its simplicity.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JdBCWFrvRbJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "CSETXAGAPyQU",
        "outputId": "93ccccc1-1546-49b6-ee5c-4efdd76b9c2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ee284789650>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://58b298081b9b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resilient Distributed Dataset (RDD**)\n",
        "* It is the basic abstraction in Spark.\n",
        "* RDD represents an immutable, partitioned collection of elements that can be operated on in parallel.\n",
        "*  The data within RDDs is segmented into logical partitions, allowing for distributed computation across multiple nodes within the cluster.\n"
      ],
      "metadata": {
        "id": "BYbW9-iM84Pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Using sparkContext.parallelize() :**\n",
        "By using parallelize() function of SparkContext (sparkContext.parallelize() ) you can create an RDD. This function loads the existing collection from your driver program into parallelizing RDD. This method of creating an RDD is used when you already have data in memory that is either loaded from a file or from a database. and all data must be present in the driver program prior to creating RDD.\n",
        "\n",
        "NOTE : For production applications, RDDs are mostly created by by using external storage systems like HDFS, S3, HBase e.t.c.\n"
      ],
      "metadata": {
        "id": "2a_39ohb8y5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDD from parallelize\n",
        "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "rdd = spark.sparkContext.parallelize(data)"
      ],
      "metadata": {
        "id": "VOH_Kw7oQByW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying contents of RDD\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kafGy2ke8vdb",
        "outputId": "768921bb-4987-4e5e-ebc6-63268f4e9ac4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'),\n",
        "(4, 5, 6, 'd e f'),\n",
        "(7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4'])"
      ],
      "metadata": {
        "id": "fVg726It89HY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the contents of RRD dataframe\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN0xDqblBMWK",
        "outputId": "ed0fe42f-c8fc-4766-daa7-071354f72a01"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+-----+\n",
            "|col1|col2|col3| col4|\n",
            "+----+----+----+-----+\n",
            "|   1|   2|   3|a b c|\n",
            "|   4|   5|   6|d e f|\n",
            "|   7|   8|   9|g h i|\n",
            "+----+----+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. By using createDataFrame( ) function**\n"
      ],
      "metadata": {
        "id": "znYWwNYcBrgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Python Spark create RDD example\") \\\n",
        "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "Employee = spark.createDataFrame([\n",
        "                ('1', 'Joe', '70000', '1'),\n",
        "                ('2', 'Henry', '80000', '2'),\n",
        "                ('3', 'Sam', '60000', '2'),\n",
        "                ('4', 'Max', '90000', '1')],\n",
        "                ['Id', 'Name', 'Sallary','DepartmentId']\n",
        ")"
      ],
      "metadata": {
        "id": "Ezbq-IlRBbZm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Employee.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqfz8DN3CQjj",
        "outputId": "95f11c45-4886-40af-b42b-4fcc12ad55ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------+------------+\n",
            "| Id| Name|Sallary|DepartmentId|\n",
            "+---+-----+-------+------------+\n",
            "|  1|  Joe|  70000|           1|\n",
            "|  2|Henry|  80000|           2|\n",
            "|  3|  Sam|  60000|           2|\n",
            "|  4|  Max|  90000|           1|\n",
            "+---+-----+-------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXvbL0BNCY2Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}